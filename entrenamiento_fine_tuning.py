# -*- coding: utf-8 -*-
"""Entrenamiento fine tuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rNqF0qlZm9r77kfDk4KveMDuiRjgYaYZ

# Entrenamiento Fine-tuning

## Configuración del entorno

Unsloth: Módulo que permite gran eficacia de fine tuning en grandes modelos
"""

# @title


!python --version
!pip install --upgrade pip
!pip install "unsloth[cu124-torch260] @ git+https://github.com/unslothai/unsloth.git"
!pip install unsloth_zoo==2025.4.4
!pip install python-dotenv # instalamos dotenv para hacer uso de los tokens del archivo .env

# @title
# modulos para el fine-tuning
from unsloth import FastLanguageModel # libreria para cargar y entrenar modelos de forma eficiente
import torch # biblioteca para deep learning
from trl import SFTTrainer # se usa para fine tuning de modelos de lenguaje
from unsloth import is_bfloat16_supported # comprueba se soporta el formato de 16 bits, que acelera entrenamientos

"""### Iniciar sesión en hugging face"""

# @title
# modulos de hugging face
from huggingface_hub import login # para poder iniciar sesion
from transformers import TrainingArguments # para definir los parametros del entrenamiento
from datasets import load_dataset # para poder cargar datasets desde hugging face

from google.colab import drive
drive.mount('/content/drive')

from dotenv import load_dotenv # carga variables desde un archivo .env
import os # maneja variables y archivos

# Carga las variables desde el archivo
#load_dotenv("/content/drive/My Drive/TFG/.env")
load_dotenv("/content/drive/Shareddrives/TFG/.env") #usad esto si no sois yo :D

# Obtiene la api
huggingface_token = os.getenv("HUGGINGFACE_API_KEY")

# Iniciar sesión con Hugging Face
from huggingface_hub import login
login(huggingface_token)


"""### Wandb"""

# @title
# Iniciamos sesion en Weights and Biases
import wandb
wandb_token = os.getenv("WANDB_API_KEY")
wandb.login(key=wandb_token)

# 8c426e8a6f583feaeebd7e002ea3f65725f6a26f

run = wandb.init( # crea un experimento en wandb
    project = 'Fine tuning DeepSeek TFG',
    job_type = 'training',
    anonymous = 'allow'
)

"""### Cargado del modelo"""

# @title
# Establecemos parametros
max_seq_length = 2048 # maxima longitud de secuencia que el modelo puede procesaer (cuantos tokens pueden ser procesados al mismo tiempo)
dtype = None # valor por defecto
load_in_4bit = True # activa la cuantizacion en 4 bit para ahorrar memoria

# Cargamos el modelo Deepseek y el tokenizer usando unsloth gracias a FastLanguageModel
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/DeepSeek-R1-Distill-Qwen-7B-unsloth-bnb-4bit",  # modelo pre entrenado de deepseek
    max_seq_length=max_seq_length, # secuencia maxima
    dtype=dtype,
    load_in_4bit=load_in_4bit, # cuantizacion 4-bit
    token=huggingface_token, # token de hugging face
)

# @title
# Definimos el prompt
prompt_style = """Below is an instruction that describes a task, paired with an input that provides further context.
Write a response that accurately and thoroughly completes the request.
Please analyze and provide a detailed solution to the following cybersecurity challenge.
Your response must include all key phases—reconnaissance, enumeration, exploitation, and privilege escalation.
For each phase, explain your reasoning behind every action, provide a step-by-step breakdown of your thought process, and suggest alternative approaches where applicable.
Before answering, think carefully about the question and create a clear, logical chain of thoughts to ensure an accurate, comprehensive, and well-structured response.

### Instruction:
You are a cybersecurity expert with advanced knowledge in penetration testing, vulnerability analysis, and exploit development.
You specialize in guiding and explaining the process of solving VulnHub machines,
focusing on practical methodologies, critical thinking, and step-by-step problem-solving strategies.

### Question:
{}

### Response:
<think>{}"""

# @title
# prompt de entrenamiento
train_prompt_style = """Below is an instruction that describes a task, paired with an input that provides further context.
Write a response that accurately and thoroughly completes the request.
Please analyze and provide a detailed solution to the following cybersecurity challenge.
Your response must include all key phases—reconnaissance, enumeration, exploitation, and privilege escalation.
For each phase, explain your reasoning behind every action, provide a step-by-step breakdown of your thought process, and suggest alternative approaches where applicable.
Before answering, think carefully about the question and create a clear, logical chain of thoughts to ensure an accurate, comprehensive, and well-structured response.

### Instruction:
You are a cybersecurity expert with advanced knowledge in penetration testing, vulnerability analysis, and exploit development.
You specialize in guiding and explaining the process of solving VulnHub machines,
focusing on practical methodologies, critical thinking, and step-by-step problem-solving strategies.

### Question:
{}

### Response:
<think>
{}
</think>
{}"""

"""### Preparamos el dataset"""

# @title
# Cargamos el dataset que ya hemos definido y subido a hugging face
dataset = load_dataset("NachoRedNav/Def-All-2", split = "train[0:1130]",trust_remote_code=True)
dataset

# @title
# Para que el modelo entienda cuándo debe finalizar una secuencia de texto
EOS_TOKEN = tokenizer.eos_token
EOS_TOKEN

# @title
# Cogemos los ejemplos y los transformamos a un formato especifico para el entrenamiento
def formatting_prompts_func(examples):  # Toma como entrada el dataset
    inputs = examples["Question"]       # Extrae la pregunta from the dataset
    cots = examples["Complex_CoT"]      # Extrae la cadena de pensamiento
    outputs = examples["Response"]      # Extracte la respuesta generada

    texts = []  # Inicializa una lista vacia para guardar los prompts con la estructura definida

    for input, cot, output in zip(inputs, cots, outputs): # Unimos las listas en tuplas e iteramos sobre ellas
        text = train_prompt_style.format(input, cot, output) + EOS_TOKEN  # insertamos los valores ademas del token EOS
        texts.append(text)  # Lo agregamos a la lista

    return {
        "text": texts,  # Devuelve el dataset con la estructura nueva con una columna "text" que contiene los prompts estructurados
    }

# @title
# Lo actualizamos
dataset_finetune = dataset.map(formatting_prompts_func, batched = True)
dataset_finetune["text"][0]

# @title
# Aplicamos LoRA (Low-Rank Adaptation) para "fine-tunear" el modelo
model_lora = FastLanguageModel.get_peft_model(
    model,
    r=16,  # LoRA rank: Determina el tamaño de los adaptores entrenables (higher = mas parametros, lower = mas eficiencia)
    target_modules=[  # Lista de capas transformer que seran aplicados
        "q_proj",   # Proyección de la consulta en el mecanismo de autoatención
        "k_proj",   # Proyección de la clave en el mecanismo de autoatención
        "v_proj",   # Proyección del valor en el mecanismo de autoatención
        "o_proj",   # Proyección de salida desde la capa de atención
        "gate_proj",  # Usado en capas de alimentación hacia adelante (MLP)
        "up_proj",    # Parte de la red de alimentación hacia adelante (FFN) del transformador
        "down_proj",  # Otra parte de la FFN del transformador
    ],
    lora_alpha=32,  # Factor de escalado para las actualizaciones de LoRA (valores más altos permiten mayor influencia de las capas LoRA)
    lora_dropout=0,  # Tasa de dropout para las capas LoRA (0 significa sin dropout, reteniendo toda la información)
    bias="none",  # Especifica si las capas LoRA deben aprender términos de sesgo (configurar en "none" ahorra memoria)
    use_gradient_checkpointing="unsloth",  # Ahorra memoria al recalcular activaciones en lugar de almacenarlas (recomendado para ajuste fino de contexto largo)
    random_state=3407,  # Establece una semilla para la reproducibilidad, asegurando el mismo comportamiento en distintos entrenamientos
    use_rslora=False,  # Indica si se debe usar Rank-Stabilized LoRA (deshabilitado aquí, por lo que se usa LoRA de rango fijo)
    loftq_config=None,  # La cuantización de fine tuning en baja precisión (LoFTQ) está deshabilitada en esta configuración
)

"""## Entrenamiento

### Parámetros
"""

# @title
# Inicializamos entrenador de fine-tuning
trainer = SFTTrainer(
    model=model_lora,  # modelo a finetunear
    tokenizer=tokenizer,  # tokenizer del modelo
    train_dataset=dataset_finetune,  # dataset de entrenamiento
    dataset_text_field="text",  # columna del dataset que contiene el texto sobre el que se va a entrenar el modelo
    max_seq_length=max_seq_length,  # longitud maxima de datos de entrada
    dataset_num_proc=2,  # numero de procesos para procesar el dataset en paralelo

    # Define argumentos de entrenamiento
    args=TrainingArguments(
        per_device_train_batch_size=4,  # numero de muestras que se procesan por cada paso
        gradient_accumulation_steps=2,  # se acumulan gradientes durante 4 pasos antes de actualizar los pesos
        #num_train_epochs=5, # numero de veces que el modelo verá todo el dataset durante el entrenamiento
        warmup_steps=25,  # numero de pasos antes de que el learning rate alcance su valor maximo
        max_steps=300,  # se entrenará durante 60 iteraciones
        learning_rate=2e-4,  ## tasa de aprendizaje
        fp16=not is_bfloat16_supported(),  # usa fp16 si es posible, que ahorra memoria
        bf16=is_bfloat16_supported(),  # usa BF16 si es posible
        logging_steps=10,  # imprime resultados cada 10 pasos
        optim="adamw_8bit",  # para optimizar memoria
        weight_decay=0.01,  # para evitar sobreajuste (overfitting)
        lr_scheduler_type="cosine",  # ajusta la tasa de aprendizaje
        seed=3407,  # fija una semilla para obtener resultados reproducibles
        output_dir="outputs",  # directorio donde se guardan los modelos y logs generados en el entrenamiento
    ),
)

"""### Entrenamiento"""

# @title
# Empezamos el entrenamiento fine tuning
trainer_stats = trainer.train()

# @title
# Guardamos el modelo
wandb.finish()

"""## Prueba y guardado"""

# @title
question = """What command should I use to delete two directories?"""

# Cargar el modelo de inferencia usando FastLanguageModel (Unsloth está optimizado para velocidad)
FastLanguageModel.for_inference(model_lora)

# Tokenizar la pregunta de entrada con un formato de prompt específico y moverla a la GPU
inputs = tokenizer([prompt_style.format(question, "")], return_tensors="pt").to("cuda")

# Generar una respuesta usando el modelo ajustado con LoRA y parámetros específicos
outputs = model_lora.generate(
    input_ids=inputs.input_ids,          # IDs tokenizados de entrada
    attention_mask=inputs.attention_mask, # Máscara de atención para manejar el padding
    max_new_tokens=1200,                  # Longitud máxima de la respuesta generada
    use_cache=True,                        # Habilitar caché para una generación eficiente
)

# Decodificar la respuesta generada del formato tokenizado a texto legible
response = tokenizer.batch_decode(outputs)

# Extraer e imprimir solo la parte de la respuesta del modelo después de "### Response:"
print(response[0].split("### Response:")[1])

# @title
trainer.save_model()
#model = model.merge_and_unload()  # Fusiona LoRA con el modelo base
#model.save_pretrained("modeloTFG")  # Guarda el modelo fusionado
tokenizer.save_pretrained("modeloTFG_7B_2")

# @title
from huggingface_hub import notebook_login

notebook_login()

# @title
from huggingface_hub import HfApi, HfFolder, Repository
from transformers import GPTNeoForCausalLM

# Cargar tu modelo entrenado
model.push_to_hub("modeloTFG_1.5B_6.1")
tokenizer.push_to_hub("modeloTFG_1.5B_6.1")

# @title
!apt-get update && apt-get install -y cmake git

# Commented out IPython magic to ensure Python compatibility.
# @title
!git clone --depth 1 https://github.com/ggerganov/llama.cpp.git
# %cd llama.cpp

# @title
model.push_to_hub_gguf(
    #"abo1515/modeloTFG_mejorado_GGUF",
    "West1125/modeloTFG_1.5B_6.1_GGUF",
    tokenizer,
    quantization_method="q4_k_m"
)