# -*- coding: utf-8 -*-
"""DSR1-RAG-Answer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gi6cTi1GQ7oiX6fzlFF4Qfh98UxU4Flo
"""

!pip install --quiet --upgrade langchain-text-splitters langchain-community langgraph langchain-google-vertexai langchain-deepseek langchain-core faiss-cpu

import os
import json
import time
import faiss
from langchain_community.docstore.in_memory import InMemoryDocstore
from langchain_community.vectorstores import FAISS
from langchain_deepseek import ChatDeepSeek
from langchain_google_vertexai import VertexAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import TextLoader
from langchain_core.vectorstores import InMemoryVectorStore
from langchain_core.documents import Document
from typing_extensions import List, TypedDict
from langgraph.graph import START, StateGraph
from google.colab import drive

drive.mount('/content/drive')

API_KEY = 'sk-6a3022d9af07439f89e265ec46ec21b1'
os.environ["DEEPSEEK_API_KEY"] = API_KEY

llm = ChatDeepSeek(
    model="deepseek-chat",
    temperature=0.5,
    max_tokens=None,
    timeout=None,
    max_retries=2,
)

credentials = '/content/drive/MyDrive/TFG-Ciber/Data-Set-Nacho/cybernetic-hue-455208-i2-d9a46af731a9.json'
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = credentials
embeddings = VertexAIEmbeddings(model="text-embedding-004")
embedding_dim = len(embeddings.embed_query("hello world"))
index = faiss.IndexFlatL2(embedding_dim)

vector_store = FAISS(
    embedding_function=embeddings,
    index=index,
    docstore=InMemoryDocstore(),
    index_to_docstore_id={},
)

file_name = 'EARTH.txt'
file_path = '/content/drive/MyDrive/TFG-Ciber/Data-Set-Nacho/Files/' + file_name

loader = TextLoader(file_path, encoding='UTF-8')
docs = loader.load()

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,  # chunk size (characters)
    chunk_overlap=100,  # chunk overlap (characters)
    add_start_index=True,  # track index in original document
)
all_splits = text_splitter.split_documents(docs)

_ = vector_store.add_documents(documents=all_splits)

system_prompt_answers =  """
You are a highly skilled cybersecurity teacher with deep expertise
in ethical hacking and penetration testing.
Your role is to teach and explain answers to questions related to hacking VulnHub machines.
You will be given a question and supporting context from a walkthrough or investigation.
Use only the information in the context to answer.
Do not mention or reference the context or its source in your response or your thought process.
If the information provided is insufficient to answer accurately, respond with:
"I don't have enough information to answer that."

Context:
{context}
Question: {question}
"""

# Define state for application
class State(TypedDict):
    question: str
    context: List[Document]
    answer: str


# Define application steps
def retrieve(state: State):
    retrieved_docs = vector_store.max_marginal_relevance_search(state["question"])
    return {"context": retrieved_docs}


def generate(state: State):
    docs_content = "\n\n".join(doc.page_content for doc in state["context"])
    messages = system_prompt_answers.format(context=docs_content, question=state["question"])
    response = llm.invoke(messages)
    result = "[Answer]: " + response.content + "\n\n"
    result += "[Explanation]: " + response.additional_kwargs["reasoning_content"]
    return {"answer": result}

graph_builder = StateGraph(State).add_sequence([retrieve, generate])
graph_builder.add_edge(START, "retrieve")
graph = graph_builder.compile()

questions_path = '/content/drive/MyDrive/TFG-Ciber/Data-Set-Nacho/Questions/' + file_name
with open(questions_path, "r", encoding="utf-8") as file:
    questions_list = [line.strip() for line in file.readlines() if line.strip()]

answers_list = []
for q in questions_list:
    res = []
    response = graph.invoke({"question": q})
    if(len(response["answer"].split("[Explanation]:")) < 2):
      continue
    answer_part = response["answer"].split("[Explanation]:")[0].replace("[Answer]:", "").strip()
    explanation_part = response["answer"].split("[Explanation]:")[1].strip()
    res.append(q)
    res.append(explanation_part)
    res.append(answer_part)
    answers_list.append(res)
    print(q)
    time.sleep(1)

converted = [
      {
          "Question": item[0],
          "Complex_CoT": item[1],
          "Response": item[2]
      }
      for item in answers_list
  ]

json_output = json.dumps(converted, indent=4)
json_file_name = file_name.split('.')[0] + '.json'
result_path = '/content/drive/MyDrive/TFG-Ciber/Data-Set-Nacho/Answers/' + json_file_name
with open(result_path, "w", encoding="utf-8") as file:
    file.write(json_output)