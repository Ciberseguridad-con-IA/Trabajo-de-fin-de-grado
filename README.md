## üìò Trabajo de fin de grado

Este proyecto en **Python** permite generar un dataset en formato **JSON** a partir de archivos **PDF sin estructura fija**, utilizando **LLM`s** para automatizar el siguiente flujo:

---

### ‚öôÔ∏è Funcionalidades del Sistema de Generaci√≥n de Dataset desde PDFs
‚úÖ Lectura y segmentaci√≥n de contenido en PDF
El sistema es capaz de procesar archivos PDF sin estructura fija, extrayendo texto de manera eficiente y dividi√©ndolo en fragmentos coherentes que representen pasos, instrucciones o secciones de inter√©s para su posterior an√°lisis.

‚úÖ Generaci√≥n autom√°tica de preguntas relevantes
A partir de los fragmentos de texto extra√≠dos, se generan autom√°ticamente preguntas pertinentes que permiten evaluar la comprensi√≥n del contenido o la resoluci√≥n de retos presentes en el documento.

‚úÖ Producci√≥n de cadenas de razonamiento ("Chain of Thought")
Para cada pregunta generada, el sistema produce una cadena de pensamiento l√≥gica que representa el proceso mental paso a paso para llegar a la respuesta, mejorando la explicabilidad del modelo.

‚úÖ Generaci√≥n de respuestas finales con contexto
Se genera una respuesta final fundamentada, basada en el contenido original del documento y en la cadena de pensamiento previa, asegurando precisi√≥n y coherencia con el material fuente.

‚úÖ Exportaci√≥n de resultados a archivo JSON
Todo el contenido generado (pregunta, pensamiento y respuesta) se guarda en un archivo .json.

---

### ‚öôÔ∏è Funcionalidades del Sistema de Fine-Tuning con Datos de Ciberseguridad

‚úÖ Carga eficiente del modelo LLM con Unsloth
Utiliza la librer√≠a Unsloth para cargar y preparar el modelo DeepSeek-R1-Distill-Qwen-7B, reduciendo el consumo de memoria y acelerando el entrenamiento mediante el uso de t√©cnicas optimizadas (como el uso de Flash Attention y carga en 4 bits).

‚úÖ Integraci√≥n de LoRA para Fine-Tuning Acelerado
Se aplica LoRA (Low-Rank Adaptation) con configuraci√≥n personalizada (r=16, alpha=16) para modificar selectivamente capas del modelo, permitiendo un entrenamiento eficiente y econ√≥mico sin necesidad de ajustar todos los par√°metros del modelo base.

‚úÖ Personalizaci√≥n del estilo de prompt
El sistema adapta el formato de los datos al estilo t√≠pico de chat (question, thought, answer)

‚úÖ Entrenamiento supervisado con SFTTrainer
Utiliza SFTTrainer de Hugging Face con t√©cnicas avanzadas:

  - Mezcla del modelo base y adaptado
  
  - Logging en Weights & Biases para visualizaci√≥n de m√©tricas
  
  - Early stopping y evaluaci√≥n con warm-up
  
  - Tokenizaci√≥n eficiente con padding din√°mico

‚úÖ Evaluaci√≥n con ejemplos personalizados
Permite evaluar f√°cilmente el rendimiento del modelo usando un conjunto de preguntas nuevas, generando predicciones paso a paso desde el modelo afinado.

‚úÖ Exportaci√≥n y carga del modelo. El modelo puede guardarse en dos formatos:

  - LoRA adaptado: √∫til si se quiere seguir entrenando o aplicar sobre el modelo base
  
  - Modelo fusionado (merge_and_unload()): para despliegue directo sin dependencia de LoRA


‚úÖ Soporte para datasets personalizados
Admite la carga de datasets en Hugging Face (NachoRedNav/Def-All-2) que contienen preguntas reales extra√≠das de procedimientos de pentesting, permitiendo un entrenamiento centrado en contextos de ciberseguridad realistas.

---

### üì¶ Apunte

En este repositorio solo se muestran los scripts, para una correcta ejecuci√≥n se deberan usar en el entorno de **Google Colab**, con las claves API's necesarias, y la configuraci√≥n pertinente.
